{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1577fde",
   "metadata": {},
   "source": [
    "ER Wait Times Scraper for HospitalStats.org\n",
    "\n",
    "This module fetches metro ER pages from HospitalStats.org, parses tabular/ semi-structured HTML into a normalized DataFrame, and outputs these to a csv.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8a00b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "import re, time, csv, argparse, datetime as dt, difflib\n",
    "from urllib.parse import urljoin\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup, NavigableString, Tag\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7bf2489",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIDWEST_ABBR = {\"IL\",\"IN\", \"IA\",\"KS\",\"MI\",\"MN\",\"MO\",\"NE\",\"ND\",\"OH\",\"SD\",\"WI\"} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3759efb2",
   "metadata": {},
   "source": [
    "Scraping the data from Chicago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "299ff34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url and headers configuration\n",
    "BASE_URL  = globals().get(\"BASE_URL\", \"https://www.hospitalstats.org\")\n",
    "METRO_URL = globals().get(\"METRO_URL\", \"https://www.hospitalstats.org/ER-Wait-Time/Chicago-IL-Metro.htm\")\n",
    "HEADERS   = globals().get(\"HEADERS\", {\"User-Agent\": \"ERWaitTimes/1.0 (+contact@example.com)\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bf9bb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rifa\\AppData\\Local\\Temp\\ipykernel_11372\\2534586742.py:2: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  STAMP      = dt.datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n"
     ]
    }
   ],
   "source": [
    "# configuration\n",
    "STAMP      = dt.datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "OUTDIR     = Path(\"out\") / STAMP\n",
    "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "DELAY = 0.8\n",
    "\n",
    "# regex patterns\n",
    "TIME_RE = re.compile(r'\\b(?:(\\d+)\\s*h)?\\s*(\\d+)\\s*m\\b', re.I)\n",
    "PCT_RE  = re.compile(r'(\\d+(?:\\.\\d+)?)\\s*%')\n",
    "WS      = re.compile(r'\\s+')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e109376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert (hour, minute) string to integer for minutes\n",
    "def to_minutes(s: str | None) -> int | None:\n",
    "    # Convert '2h 15m' or '54m' to minutes, 'N/A' -> None\n",
    "    if s is None:\n",
    "        return None\n",
    "    s = str(s)\n",
    "    if \"N/A\" in s:\n",
    "        return None\n",
    "    m = TIME_RE.search(s)\n",
    "    if not m:\n",
    "        return None\n",
    "    h = int(m.group(1) or 0)\n",
    "    mins = int(m.group(2))\n",
    "    return h * 60 + mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5987b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# strips and normalizes whitespace\n",
    "def clean(s): \n",
    "    return WS.sub(\" \", (s or \"\").strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "239c218f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer to percent \n",
    "def to_percent(s):\n",
    "    if not s: return None\n",
    "    m = PCT_RE.search(str(s))\n",
    "    return float(m.group(1)) if m else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1e5304d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET a URL using a shared Session with retry/backoff and polite headers\n",
    "def fetch_html(url: str) -> str:\n",
    "    with requests.Session() as s:\n",
    "        s.headers.update(HEADERS)\n",
    "        r = s.get(url, timeout=25)\n",
    "        r.raise_for_status()\n",
    "        return r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb402d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try fast-path parsing with pandas.read_html for well-formed tables\n",
    "def try_read_html_tables(html: str) -> pd.DataFrame | None:\n",
    "    \"\"\"\n",
    "    Try to parse any tables directly with pandas.\n",
    "    Returns a DataFrame with at least hospital name + wait text if possible.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tables = pd.read_html(html) \n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "    if not tables:\n",
    "        return None\n",
    "\n",
    "    # look for a table that lists hospitals & times\n",
    "    candidates = []\n",
    "    for t in tables:\n",
    "        cols_lower = [str(c).lower() for c in t.columns]\n",
    "        if any(\"hospital\" in c for c in cols_lower) or any(\"wait\" in c for c in cols_lower):\n",
    "            candidates.append(t)\n",
    "\n",
    "    if not candidates:\n",
    "        return None\n",
    "\n",
    "    # Take the largest table\n",
    "    df = max(candidates, key=len).copy()\n",
    "    df.columns = [str(c).strip().lower().replace(\" \", \"_\") for c in df.columns]\n",
    "    name_col = next((c for c in df.columns if \"hospital\" in c or c == \"name\"), None)\n",
    "\n",
    "    if name_col is None:\n",
    "        return None\n",
    "\n",
    "    # Reduce to a minimal schema\n",
    "    out = pd.DataFrame({\n",
    "        \"hospital_name\": df[name_col].astype(str).str.strip(),\n",
    "    })\n",
    "\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5d927d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_text(n) -> str:\n",
    "    # Return visible text for any BeautifulSoup node (Tag or string)\n",
    "    if n is None:\n",
    "        return \"\"\n",
    "    if isinstance(n, NavigableString):\n",
    "        return str(n)\n",
    "    if hasattr(n, \"get_text\"):\n",
    "        return n.get_text(\" \", strip=True)\n",
    "    return str(n)\n",
    "\n",
    "def safe_join(parts, sep=\" \"):\n",
    "    # Join any list/iterable of nodes/strings safely as text\n",
    "    return sep.join([node_text(p) for p in parts if p is not None and node_text(p)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74f4f808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bs4_fallback_extract(html: str) -> pd.DataFrame:\n",
    "    \"\"\" Parse a metro page using BeautifulSoup when read_html is unreliable.\n",
    "    Parameters: \n",
    "        html : str\n",
    "            Raw HTML for a metro ER page (e.g., Chicago-IL-Metro).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame\n",
    "            Frame with normalized columns such as:\n",
    "            - 'Hospital'\n",
    "            - 'City'\n",
    "            - 'State'\n",
    "            - 'WaitMinutes' (int or None)\n",
    "            - 'LeftBeforeSeen' (float % or None)\n",
    "            - 'NotRecommended' (float % or None) \"\"\"\n",
    "    if BeautifulSoup is None:\n",
    "        raise RuntimeError(\"BeautifulSoup not installed; `pip install beautifulsoup4` or rely on read_html tables.\")\n",
    "\n",
    "    from urllib.parse import urljoin\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Hospital links: /hospital-ratings/....\n",
    "    links = soup.select('a[href*=\"/hospital-ratings/\"]')\n",
    "    rows = []\n",
    "    # \"Comment <City> <Wait/N/A>\"\n",
    "    row_re = re.compile(r'Comment\\s+(.*?)\\s+(?:(\\d+\\s*h\\s*\\d+\\s*m|\\d+\\s*m)|N/A)\\b', re.I)\n",
    "\n",
    "    for a in links:\n",
    "        name = a.get_text(strip=True)\n",
    "        href = urljoin(BASE_URL, a.get(\"href\",\"\").strip())\n",
    "\n",
    "        parent_text = a.parent.get_text(\" \", strip=True)\n",
    "        tail = parent_text.replace(name, \"\", 1).strip()\n",
    "        m = row_re.search(tail)\n",
    "\n",
    "        city = m.group(1).strip() if m else None\n",
    "        wait_text = (m.group(2).strip() if (m and m.group(2)) else \"N/A\")\n",
    "\n",
    "        rows.append({\n",
    "            \"hospital_name\": name,\n",
    "            \"city\": city,\n",
    "            \"wait_text\": wait_text,\n",
    "            \"wait_minutes\": to_minutes(wait_text),\n",
    "            \"detail_url\": href,          \n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be0bb603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the text that follows a bold/label element within a soup tree\n",
    "def text_after_b(soup, label_regex):\n",
    "    \"\"\"\n",
    "    Find <b>Label:</b> VALUE in the same parent; collect text from following\n",
    "    siblings up to the first <br>. Robust to Tag objects.\n",
    "    Parameters:\n",
    "        soup: bs4.BeautifulSoup | Tag\n",
    "        label_regex: Pattern[str]\n",
    "    \"\"\"\n",
    "    for b in soup.find_all(\"b\"):\n",
    "        if re.search(label_regex, b.get_text(\" \", strip=True), re.I):\n",
    "            parts = []\n",
    "            for sib in b.next_siblings:\n",
    "                if isinstance(sib, Tag) and sib.name == \"br\":\n",
    "                    break\n",
    "                parts.append(sib)\n",
    "            txt = clean(safe_join(parts))\n",
    "            if txt:\n",
    "                return txt\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5317926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a normalized key for hospital names to improve joins across pages\n",
    "def norm_name(s: str) -> str:\n",
    "    if not s: return \"\"\n",
    "    s = s.strip()\n",
    "    s = re.sub(r'(?i)\\s*comment\\s*$', '', s)  \n",
    "    s = re.sub(r'\\s+', ' ', s)             \n",
    "    s = re.sub(r\"[’'`]\", \"\", s)         \n",
    "    return s.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5df54ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_or_na(frame: pd.DataFrame, col: str) -> pd.Series:\n",
    "    # Return frame[col] if it exists, else a NA Series aligned to frame.index\n",
    "    return frame[col] if col in frame.columns else pd.Series(pd.NA, index=frame.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9853fe45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotate the metro DataFrame with per-hospital detail page URLs\n",
    "def attach_detail_urls(html: str, df: pd.DataFrame) -> pd.DataFrame:\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    anchors = soup.select('a[href*=\"/hospital-ratings/\"]')\n",
    "    link_rows = []\n",
    "    for a in anchors:\n",
    "        text = a.get_text(strip=True)\n",
    "        href = (a.get(\"href\") or \"\").strip()\n",
    "        if text and href:\n",
    "            link_rows.append({\n",
    "                \"__key\": norm_name(text),\n",
    "                \"anchor_name\": text,\n",
    "                \"detail_url_from_anchor\": urljoin(BASE_URL, href),\n",
    "            })\n",
    "    df_links = pd.DataFrame(link_rows).drop_duplicates(subset=\"__key\")\n",
    "\n",
    "    # Clean names and key\n",
    "    if \"hospital_name\" not in df.columns:\n",
    "        raise RuntimeError(\"Need a 'hospital_name' column on df.\")\n",
    "    df = df.copy()\n",
    "    df[\"hospital_name\"] = (\n",
    "        df[\"hospital_name\"].astype(str)\n",
    "          .str.replace(r'(?i)\\s*comment\\s*$', '', regex=True)\n",
    "          .str.strip()\n",
    "    )\n",
    "    df[\"__key\"] = df[\"hospital_name\"].map(norm_name)\n",
    "\n",
    "    # Safe merge\n",
    "    if \"detail_url\" in df.columns:\n",
    "        df = df.rename(columns={\"detail_url\": \"detail_url_existing\"})\n",
    "\n",
    "    df = df.merge(df_links[[\"__key\", \"anchor_name\", \"detail_url_from_anchor\"]],\n",
    "                  on=\"__key\", how=\"left\")\n",
    "    df[\"hospital_name\"] = df[\"anchor_name\"].fillna(df[\"hospital_name\"])\n",
    "    existing = series_or_na(df, \"detail_url_existing\")\n",
    "    df[\"detail_url\"] = df[\"detail_url_from_anchor\"].where(df[\"detail_url_from_anchor\"].notna(), existing)\n",
    "\n",
    "    # Cleanup\n",
    "    for col in [\"anchor_name\", \"detail_url_from_anchor\", \"detail_url_existing\"]:\n",
    "        if col in df.columns:\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2dd6ea47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rifa\\AppData\\Local\\Temp\\ipykernel_22792\\4095769453.py:5: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  tables = pd.read_html(html)\n",
      "C:\\Users\\Rifa\\AppData\\Local\\Temp\\ipykernel_22792\\4095769453.py:72: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  stamp = dt.datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n"
     ]
    }
   ],
   "source": [
    "html = requests.get(METRO_URL, headers=HEADERS, timeout=25).text\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# read tables and pick the one with a Hospital column\n",
    "tables = pd.read_html(html)\n",
    "base = None\n",
    "for t in tables:\n",
    "    cols = [str(c).lower() for c in t.columns.astype(str)]\n",
    "    if any(\"hospital\" in c for c in cols):\n",
    "        base = t.copy(); break\n",
    "if base is None:\n",
    "    raise RuntimeError(\"Could not find the hospital table on the metro page.\")\n",
    "\n",
    "# normalize and keep core fields\n",
    "base.columns = [str(c).strip().lower().replace(\"name\", \"hospital_name\") for c in base.columns]\n",
    "name_col = [c for c in base.columns if \"hospital\" in c][0]\n",
    "city_col = next((c for c in base.columns if \"city\" in c), None)\n",
    "wait_col = next((c for c in base.columns if \"wait\" in c and \"time\" in c), None)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"hospital_name\": base[name_col].astype(str).str.strip(),\n",
    "    \"city\": base[city_col] if city_col else pd.NA,\n",
    "    \"wait_text\": base[wait_col] if wait_col else pd.NA,\n",
    "})\n",
    "df[\"wait_minutes\"] = df[\"wait_text\"].apply(to_minutes)\n",
    "\n",
    "# attach detail_url from anchors\n",
    "anchors = soup.select('a[href*=\"/hospital-ratings/\"]')\n",
    "link_rows = []\n",
    "for a in anchors:\n",
    "    text = a.get_text(strip=True)\n",
    "    href = (a.get(\"href\") or \"\").strip()\n",
    "    if text and href:\n",
    "        link_rows.append({\n",
    "            \"__key\": norm_name(text),\n",
    "            \"anchor_name\": text,\n",
    "            \"detail_url\": urljoin(BASE_URL, href),\n",
    "        })\n",
    "df_links = pd.DataFrame(link_rows).drop_duplicates(subset=\"__key\")\n",
    "\n",
    "df[\"hospital_name\"] = df[\"hospital_name\"].str.replace(r'(?i)\\s*comment\\s*$', '', regex=True).str.strip()\n",
    "df[\"__key\"] = df[\"hospital_name\"].map(norm_name)\n",
    "df = df.merge(df_links, on=\"__key\", how=\"left\")\n",
    "df[\"hospital_name\"] = df[\"anchor_name\"].fillna(df[\"hospital_name\"])\n",
    "df.drop(columns=[\"anchor_name\",\"__key\"], inplace=True)\n",
    "\n",
    "# write staging + timeseries\n",
    "now_iso = dt.datetime.now(dt.timezone.utc).isoformat()\n",
    "df[\"scrape_ts\"] = now_iso\n",
    "df[\"source_url\"] = METRO_URL\n",
    "\n",
    "staging = pd.DataFrame({\n",
    "    \"source\": \"hospitalstats\",\n",
    "    \"name\": df[\"hospital_name\"],\n",
    "    \"city\": df[\"city\"],\n",
    "    \"wait_minutes\": df[\"wait_minutes\"].astype(\"Int64\"),\n",
    "    \"source_url\": df[\"source_url\"],\n",
    "    \"detail_url\": df[\"detail_url\"],\n",
    "    \"raw_wait_text\": df[\"wait_text\"],\n",
    "    \"scrape_ts\": df[\"scrape_ts\"],\n",
    "})\n",
    "\n",
    "timeseries = pd.DataFrame({\n",
    "    \"source\": staging[\"source\"],\n",
    "    \"name\": staging[\"name\"],\n",
    "    \"city\": staging[\"city\"],\n",
    "    \"ts\": staging[\"scrape_ts\"],\n",
    "    \"wait_minutes\": staging[\"wait_minutes\"],\n",
    "    \"source_url\": staging[\"source_url\"],\n",
    "})\n",
    "\n",
    "stamp = dt.datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "outdir = Path(\"out\") / stamp\n",
    "outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "staging.to_csv(outdir / \"staging_chicago.csv\", index=False)\n",
    "timeseries.to_csv(outdir / \"wait_times_timeseries.csv\", index=False)\n",
    "staging.to_csv(\"staging_chicago.csv\", index=False)\n",
    "timeseries.to_csv(\"wait_times_timeseries.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c348ad13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract additional metrics from a hospital's detail page:\n",
    "def parse_detail_page(html: str, url: str) -> dict:\n",
    "    \"\"\" \n",
    "        address, phone, hospital type, emergency services,\n",
    "        mortality rates (overall, heart attack, stroke, heart failure, pneumonia),\n",
    "        infection cases (C. Diff, MRSA),\n",
    "        average ER wait time,\n",
    "        patient ratings (overall, positive points, negative points)\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    h1 = soup.find([\"h1\",\"h2\"])\n",
    "    name = clean(h1.get_text(\" \", strip=True)) if h1 else None\n",
    "\n",
    "    # address/phone block\n",
    "    address=city=state=postal=phone=None\n",
    "    left = soup.select('div[style*=\"float:left\"][style*=\"width:40%\"]')\n",
    "    if left:\n",
    "        addr_html = left[0]\n",
    "        lines = [x for x in addr_html.get_text(\"\\n\", strip=True).split(\"\\n\") if x]\n",
    "        if lines: address = clean(lines[0])\n",
    "        if len(lines)>=2:\n",
    "            m = re.search(r\"(.+?),\\s*([A-Z]{2})\\s+(\\d{5}(?:-\\d{4})?)\", lines[1])\n",
    "            if m: city, state, postal = clean(m.group(1)), m.group(2), m.group(3)\n",
    "        b_phone = addr_html.find(\"b\", string=re.compile(r\"^\\s*Phone\\s*:\\s*$\", re.I))\n",
    "        if b_phone:\n",
    "            parts = []\n",
    "            for sib in b_phone.next_siblings:\n",
    "                if isinstance(sib, Tag) and sib.name == \"br\":\n",
    "                    break\n",
    "                parts.append(sib)\n",
    "            maybe_phone = safe_join(parts)          \n",
    "            pm = re.search(r\"\\(?\\d{3}\\)?[ -]?\\d{3}[ -]?\\d{4}\", maybe_phone)\n",
    "            if pm:\n",
    "                phone = pm.group(0)\n",
    "\n",
    "    hosp_type = text_after_b(soup, r\"Hospital\\s*Type\")\n",
    "    emergency_services = text_after_b(soup, r\"Emergency\\s*Services\")\n",
    "    if emergency_services:\n",
    "        es = emergency_services.upper()\n",
    "        emergency_services = \"YES\" if \"YES\" in es else (\"NO\" if \"NO\" in es else emergency_services)\n",
    "\n",
    "    # quality section\n",
    "    quality_hdr = soup.find(id=\"Quality\")\n",
    "    mort_text=mort_pct=mort_dir=None\n",
    "    ha=st=hf=pn=None\n",
    "    if quality_hdr:\n",
    "        span = quality_hdr.find_next(\"span\", class_=\"bigstat\")\n",
    "        if span:\n",
    "            mort_text = clean(span.get_text(\" \", strip=True))\n",
    "            m = re.search(r\"(\\d+(?:\\.\\d+)?)\\s*%\", mort_text);  mort_pct = float(m.group(1)) if m else None\n",
    "            d = re.search(r\"\\b(better|worse)\\b\", mort_text, re.I); mort_dir = d.group(1).lower() if d else None\n",
    "        tbl = quality_hdr.find_next(\"table\")\n",
    "        if tbl:\n",
    "            for tr in tbl.find_all(\"tr\"):\n",
    "                th = tr.find(\"th\"); tds = tr.find_all(\"td\")\n",
    "                if not th or not tds: continue\n",
    "                label = clean(th.get_text(\" \", strip=True))\n",
    "                pct = to_percent(tds[0].get_text(\" \", strip=True))\n",
    "                if   re.search(r\"Heart Attack\", label, re.I): ha = pct\n",
    "                elif re.search(r\"Stroke\", label, re.I):       st = pct\n",
    "                elif re.search(r\"Heart Failure\", label, re.I): hf = pct\n",
    "                elif re.search(r\"Pneumonia\", label, re.I):     pn = pct\n",
    "\n",
    "    # infections\n",
    "    c_diff=mrsa=None\n",
    "    inf = soup.find(id=\"infectious\")\n",
    "    if inf:\n",
    "        tbl = inf.find_next(\"table\")\n",
    "        if tbl:\n",
    "            for tr in tbl.find_all(\"tr\"):\n",
    "                tds = tr.find_all(\"td\")\n",
    "                if len(tds)!=2: continue\n",
    "                label = tds[0].get_text(\" \", strip=True)\n",
    "                val = re.sub(r\"[^\\d]\",\"\", tds[1].get_text(\" \", strip=True)) or None\n",
    "                val = int(val) if val else None\n",
    "                if re.search(r\"C\\.\\s*Diff\", label, re.I): c_diff = val\n",
    "                if re.search(r\"MRSA\", label, re.I):       mrsa   = val\n",
    "\n",
    "    # ER wait\n",
    "    avg_ed_min=None\n",
    "    er = soup.find(id=\"erwait\")\n",
    "    if er:\n",
    "        span = (er.find_parent() or er).find(\"span\",\"bigstat\")\n",
    "        if span: avg_ed_min = to_minutes(span.get_text(\" \", strip=True))\n",
    "\n",
    "    # patient ratings\n",
    "    overall_patient_rating = None\n",
    "    positive_points = negative_points = None\n",
    "\n",
    "    pr_hdr = soup.find(id=\"patientratings\")\n",
    "    if pr_hdr:\n",
    "        # Overall rating \n",
    "        span = pr_hdr.find_next(\"span\", class_=\"bigstat\")\n",
    "        if span:\n",
    "            overall_patient_rating = clean(span.get_text(\" \", strip=True))\n",
    "\n",
    "        # positive box\n",
    "        pos_h3 = soup.find(\"h3\", string=re.compile(r\"Positive\\s+Patient\\s+Ratings\", re.I))\n",
    "        if pos_h3:\n",
    "            pos_box = pos_h3.find_parent(\"div\")\n",
    "            if pos_box:\n",
    "                ul = pos_box.find(\"ul\") \n",
    "                if ul and ul.find_all(\"li\"):\n",
    "                    positive_points = \"; \".join(\n",
    "                        clean(li.get_text(\" \", strip=True)) for li in ul.find_all(\"li\")\n",
    "                    )\n",
    "                else:\n",
    "                    # Some pages have no <ul>, just \"No consistently positive ratings\"\n",
    "                    raw = clean(pos_box.get_text(\" \", strip=True))\n",
    "                    raw = re.sub(r\"^\\s*Positive\\s+Patient\\s+Ratings\\s*\", \"\", raw, flags=re.I).strip()\n",
    "                    if re.search(r\"No\\s+consistently\\s+positive\\s+ratings\", raw, re.I):\n",
    "                        positive_points = None  \n",
    "                    elif raw:\n",
    "                        positive_points = raw  \n",
    "\n",
    "        # negative box \n",
    "        neg_h3 = soup.find(\"h3\", string=re.compile(r\"Negative\\s+Patient\\s+Ratings\", re.I))\n",
    "        if neg_h3:\n",
    "            neg_box = neg_h3.find_parent(\"div\")\n",
    "            if neg_box:\n",
    "                ul = neg_box.find(\"ul\")\n",
    "                if ul and ul.find_all(\"li\"):\n",
    "                    negative_points = \"; \".join(\n",
    "                        clean(li.get_text(\" \", strip=True)) for li in ul.find_all(\"li\")\n",
    "                    )\n",
    "\n",
    "        # Safety: if positive == negative (selector leak), blank out positive\n",
    "        if positive_points and negative_points and positive_points == negative_points:\n",
    "            positive_points = None\n",
    "\n",
    "    return {\n",
    "        \"detail_url\": url,\n",
    "        \"detail_name\": name,\n",
    "        \"detail_address\": address, \"detail_city\": city, \"detail_state\": state, \"detail_zip\": postal, \"detail_phone\": phone,\n",
    "        \"detail_hospital_type\": hosp_type,\n",
    "        \"detail_emergency_services\": emergency_services,\n",
    "        \"detail_mortality_overall_text\": mort_text,\n",
    "        \"detail_mortality_overall_percent\": mort_pct,\n",
    "        \"detail_mortality_overall_direction\": mort_dir,\n",
    "        \"detail_mortality_heart_attack_percent\": ha,\n",
    "        \"detail_mortality_stroke_percent\": st,\n",
    "        \"detail_mortality_heart_failure_percent\": hf,\n",
    "        \"detail_mortality_pneumonia_percent\": pn,\n",
    "        \"detail_c_diff_cases\": c_diff,\n",
    "        \"detail_mrsa_cases\": mrsa,\n",
    "        \"detail_avg_time_in_ed_minutes\": avg_ed_min,\n",
    "        \"detail_overall_patient_rating\": overall_patient_rating,\n",
    "        \"detail_positive_patient_ratings\": positive_points,\n",
    "        \"detail_negative_patient_ratings\": negative_points,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1a9b0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/30] https://www.hospitalstats.org/hospital-ratings/presence-saint-joseph-hospital--chicago-chicago-il.htm\n",
      "[2/30] https://www.hospitalstats.org/hospital-ratings/loretto-hospital-chicago-il.htm\n",
      "[3/30] https://www.hospitalstats.org/hospital-ratings/thorek-memorial-hospital-chicago-il.htm\n",
      "[4/30] https://www.hospitalstats.org/hospital-ratings/insight-hospital-and-medical-center-chicago-chicago-il.htm\n",
      "[5/30] https://www.hospitalstats.org/hospital-ratings/holy-cross-hospital-chicago-il.htm\n",
      "[6/30] https://www.hospitalstats.org/hospital-ratings/jackson-park-hospital-chicago-il.htm\n",
      "[7/30] https://www.hospitalstats.org/hospital-ratings/humboldt-park-health-chicago-il.htm\n",
      "[8/30] https://www.hospitalstats.org/hospital-ratings/provident-hospital-of-chicago-chicago-il.htm\n",
      "[9/30] https://www.hospitalstats.org/hospital-ratings/saint-anthony-hospital-chicago-il.htm\n",
      "[10/30] https://www.hospitalstats.org/hospital-ratings/methodist-hospital-of-chicago-chicago-il.htm\n",
      "[11/30] https://www.hospitalstats.org/hospital-ratings/mt-sinai-hospital-medical-center-chicago-il.htm\n",
      "[12/30] https://www.hospitalstats.org/hospital-ratings/presence-saints-mary-and-elizabeth-medical-center-chicago-il.htm\n",
      "[13/30] https://www.hospitalstats.org/hospital-ratings/community-first-medical-center-chicago-il.htm\n",
      "[14/30] https://www.hospitalstats.org/hospital-ratings/louis-a-weiss-memorial-hospital-chicago-il.htm\n",
      "[15/30] https://www.hospitalstats.org/hospital-ratings/advocate-illinois-masonic-medical-center-chicago-il.htm\n",
      "[16/30] https://www.hospitalstats.org/hospital-ratings/amita-health-resurrection-medical-center-chicago-il.htm\n",
      "[17/30] https://www.hospitalstats.org/hospital-ratings/roseland-community-hospital-chicago-il.htm\n",
      "[18/30] https://www.hospitalstats.org/hospital-ratings/advocate-trinity-hospital-chicago-il.htm\n",
      "[19/30] https://www.hospitalstats.org/hospital-ratings/south-shore-hospital-chicago-il.htm\n",
      "[20/30] https://www.hospitalstats.org/hospital-ratings/swedish-hospital-chicago-il.htm\n",
      "[21/30] https://www.hospitalstats.org/hospital-ratings/john-h-stroger-jr-hospital-chicago-il.htm\n",
      "[22/30] https://www.hospitalstats.org/hospital-ratings/the-university-of-chicago-medical-center-chicago-il.htm\n",
      "[23/30] https://www.hospitalstats.org/hospital-ratings/st-bernard-hospital-chicago-il.htm\n",
      "[24/30] https://www.hospitalstats.org/hospital-ratings/rush-university-medical-center-chicago-il.htm\n",
      "[25/30] https://www.hospitalstats.org/hospital-ratings/university-of-illinois-hospital-and-clinics-chicago-il.htm\n",
      "[26/30] https://www.hospitalstats.org/hospital-ratings/northwestern-memorial-hospital-chicago-il.htm\n",
      "[27/30] https://www.hospitalstats.org/hospital-ratings/jesse-brown-va-medical-center--va-chicago-healthcare-system-chicago-il.htm\n",
      "[28/30] https://www.hospitalstats.org/hospital-ratings/ann-robert-h-lurie-childrens-hospital-of-chicago-chicago-il.htm\n",
      "[29/30] https://www.hospitalstats.org/hospital-ratings/hartgrove-hospital-chicago-il.htm\n",
      "[30/30] https://www.hospitalstats.org/hospital-ratings/montrose-behavioral-health-hospital-chicago-il.htm\n"
     ]
    }
   ],
   "source": [
    "# visit each row's 'detail_url' (if present) and enrich the DataFrame in place\n",
    "def enrich_df_inline(df: pd.DataFrame, delay: float = 0.8) -> pd.DataFrame:\n",
    "    \"\"\" \n",
    "    Parameters\n",
    "        df : pd.DataFrame\n",
    "            Frame expected to contain a 'detail_url' column with absolute URLs.\n",
    "        delay : float, default 0.8\n",
    "            Sleep duration between requests to avoid overloading the site.\n",
    "    Returns\n",
    "        pd.DataFrame\n",
    "            A new DataFrame with added/updated columns from detail pages.\n",
    "    \"\"\"\n",
    "    if \"detail_url\" not in df.columns:\n",
    "        raise ValueError(\"DataFrame must contain a 'detail_url' column.\")\n",
    "    urls = df[\"detail_url\"].dropna().astype(str).unique().tolist()\n",
    "    rows = []\n",
    "    with requests.Session() as s:\n",
    "        s.headers.update(HEADERS)\n",
    "        for i, url in enumerate(urls, 1):\n",
    "            try:\n",
    "                time.sleep(delay)\n",
    "                r = s.get(url, timeout=30)\n",
    "                r.raise_for_status()\n",
    "                rows.append(parse_detail_page(r.text, url))\n",
    "            except Exception as e:\n",
    "                rows.append({\"detail_url\": url, \"detail_error\": str(e)})\n",
    "            print(f\"[{i}/{len(urls)}] {url}\")\n",
    "    df_detail = pd.DataFrame(rows)\n",
    "    return df.merge(df_detail, on=\"detail_url\", how=\"left\")\n",
    "\n",
    "df_staging = pd.read_csv(\"staging_chicago.csv\")\n",
    "df_enriched = enrich_df_inline(df_staging, delay=0.8)\n",
    "df_enriched.to_csv(\"staging_chicago_enriched.csv\", index=False)\n",
    "df_enriched.to_csv(outdir / \"staging_chicago_enriched.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e589581a",
   "metadata": {},
   "source": [
    "Scraping for the Midwest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f78b80e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state_abbr</th>\n",
       "      <th>county_name</th>\n",
       "      <th>county_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IA</td>\n",
       "      <td>Adair</td>\n",
       "      <td>https://www.hospitalstats.org/Adair-County-IA.htm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IA</td>\n",
       "      <td>Adams</td>\n",
       "      <td>https://www.hospitalstats.org/Adams-County-IA.htm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IA</td>\n",
       "      <td>Allamakee</td>\n",
       "      <td>https://www.hospitalstats.org/Allamakee-County...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IA</td>\n",
       "      <td>Appanoose</td>\n",
       "      <td>https://www.hospitalstats.org/Appanoose-County...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IA</td>\n",
       "      <td>Audubon</td>\n",
       "      <td>https://www.hospitalstats.org/Audubon-County-I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>IA</td>\n",
       "      <td>Black Hawk</td>\n",
       "      <td>https://www.hospitalstats.org/Black-Hawk-Count...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>IA</td>\n",
       "      <td>Boone</td>\n",
       "      <td>https://www.hospitalstats.org/Boone-County-IA.htm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>IA</td>\n",
       "      <td>Bremer</td>\n",
       "      <td>https://www.hospitalstats.org/Bremer-County-IA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>IA</td>\n",
       "      <td>Buchanan</td>\n",
       "      <td>https://www.hospitalstats.org/Buchanan-County-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>IA</td>\n",
       "      <td>Buena Vista</td>\n",
       "      <td>https://www.hospitalstats.org/Buena-Vista-Coun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  state_abbr  county_name                                         county_url\n",
       "0         IA        Adair  https://www.hospitalstats.org/Adair-County-IA.htm\n",
       "1         IA        Adams  https://www.hospitalstats.org/Adams-County-IA.htm\n",
       "2         IA    Allamakee  https://www.hospitalstats.org/Allamakee-County...\n",
       "3         IA    Appanoose  https://www.hospitalstats.org/Appanoose-County...\n",
       "4         IA      Audubon  https://www.hospitalstats.org/Audubon-County-I...\n",
       "5         IA   Black Hawk  https://www.hospitalstats.org/Black-Hawk-Count...\n",
       "6         IA        Boone  https://www.hospitalstats.org/Boone-County-IA.htm\n",
       "7         IA       Bremer  https://www.hospitalstats.org/Bremer-County-IA...\n",
       "8         IA     Buchanan  https://www.hospitalstats.org/Buchanan-County-...\n",
       "9         IA  Buena Vista  https://www.hospitalstats.org/Buena-Vista-Coun..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_URL_MIDWEST = \"https://www.hospitalstats.org/ER-Wait-Time/\"\n",
    "\n",
    "def get_midwest_state_links(base_url: str = BASE_URL_MIDWEST) -> pd.DataFrame:\n",
    "    \"\"\"Build the table: state_abbr + absolute link to '<ABBR>-Counties.htm'.\"\"\"\n",
    "    r = requests.get(base_url, timeout=30, headers={\"User-Agent\": \"MidwestERScraper/1.0\"})\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "    # Find the section with the state list\n",
    "    container = None\n",
    "    for h3 in soup.find_all(\"h3\"):\n",
    "        if \"Browse Emergency Room Stats by State\" in h3.get_text(strip=True):\n",
    "            container = h3.parent\n",
    "            break\n",
    "    anchors = container.find_all(\"a\", href=True) if container else soup.find_all(\"a\", href=True)\n",
    "\n",
    "    rows = []\n",
    "    for a in anchors:\n",
    "        href = a[\"href\"]\n",
    "        # Looks like \"IL-Counties.htm\"\n",
    "        if re.fullmatch(r\"[A-Z]{2}-Counties\\.htm\", href):\n",
    "            abbr = a.get_text(strip=True).upper() or href.split(\"-\")[0]\n",
    "            if abbr in MIDWEST_ABBR:\n",
    "                rows.append({\n",
    "                    \"state_abbr\": abbr,\n",
    "                    \"midwest_state_link\": urljoin(base_url, href)\n",
    "                })\n",
    "    return pd.DataFrame(rows).drop_duplicates().sort_values(\"state_abbr\").reset_index(drop=True)\n",
    "\n",
    "def discover_counties_for_state(state_abbr: str, state_url: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For a given state '<ABBR>-Counties.htm' page, return county name + absolute URL.\n",
    "    It targets anchors like:\n",
    "        <a href=\"Ada-County-ID.htm\">Ada</a>\n",
    "    \"\"\"\n",
    "    r = requests.get(state_url, timeout=30, headers={\"User-Agent\": \"MidwestERScraper/1.0\"})\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "    # Focus on the main content area if present; fallback to whole page\n",
    "    content = soup.find(id=\"content\") or soup\n",
    "    out = []\n",
    "    for a in content.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"]\n",
    "        name = a.get_text(\" \", strip=True)\n",
    "        # County links typically contain \"-County-\" and end with \".htm\"\n",
    "        if \"-County-\" in href and href.lower().endswith(\".htm\"):\n",
    "            out.append({\n",
    "                \"state_abbr\": state_abbr,\n",
    "                \"county_name\": name,\n",
    "                \"county_url\": urljoin(BASE_URL, href)\n",
    "            })\n",
    "    return pd.DataFrame(out).drop_duplicates(subset=[\"county_url\"]).reset_index(drop=True)\n",
    "\n",
    "def build_midwest_counties_df(include_county_page_html: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    1) Gets the Midwest states table (abbr + link)\n",
    "    2) Visits each state page to list counties (name + absolute URL)\n",
    "    3) Optionally fetches every county page HTML into a 'county_html' column\n",
    "    \"\"\"\n",
    "    df_states = get_midwest_state_links()\n",
    "    all_parts = []\n",
    "    for _, row in df_states.iterrows():\n",
    "        part = discover_counties_for_state(row[\"state_abbr\"], row[\"midwest_state_link\"])\n",
    "        all_parts.append(part)\n",
    "    df_counties = pd.concat(all_parts, ignore_index=True) if all_parts else pd.DataFrame(\n",
    "        columns=[\"state_abbr\",\"county_name\",\"county_url\"]\n",
    "    )\n",
    "\n",
    "    if include_county_page_html and not df_counties.empty:\n",
    "        htmls = []\n",
    "        for u in df_counties[\"county_url\"].tolist():\n",
    "            rr = requests.get(u, timeout=30, headers={\"User-Agent\": \"MidwestERScraper/1.0\"})\n",
    "            rr.raise_for_status()\n",
    "            htmls.append(rr.text)\n",
    "        df_counties[\"county_html\"] = htmls\n",
    "\n",
    "    return df_counties\n",
    "\n",
    "# Table of Midwest counties with names + absolute URLs:\n",
    "df_midwest_counties = build_midwest_counties_df(include_county_page_html=False)\n",
    "df_midwest_counties.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4292548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_county(county_url: str, county_html: str | None = None, delay: float = 0.8):\n",
    "    \"\"\"\n",
    "    Return an enriched DataFrame for one county by:\n",
    "      1) parsing hospitals from county HTML (read_html fast-path, else BS4 fallback)\n",
    "      2) attaching detail page URLs\n",
    "      3) visiting each detail page to enrich metrics\n",
    "    \"\"\"\n",
    "    html = county_html if county_html is not None else fetch_html(county_url)\n",
    "\n",
    "    # 1) county hospitals list\n",
    "    df = try_read_html_tables(html)\n",
    "    if df is None or getattr(df, \"empty\", True):\n",
    "        df = bs4_fallback_extract(html)\n",
    "\n",
    "    # 2) attach detail page links & 3) enrich in-line\n",
    "    df = attach_detail_urls(html, df)\n",
    "    df_enriched = enrich_df_inline(df, delay=delay)\n",
    "    return df_enriched\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cd0765",
   "metadata": {},
   "outputs": [],
   "source": [
    "county_frames = {}  \n",
    "errors = []\n",
    "\n",
    "for i, row in df_midwest_counties.iterrows():\n",
    "    state_abbr = row[\"state_abbr\"]\n",
    "    county_name = row[\"county_name\"]\n",
    "    county_url  = row[\"county_url\"]\n",
    "    county_html = row.get(\"county_html\", None)\n",
    "\n",
    "    key = (state_abbr, county_name)\n",
    "    try:\n",
    "        df_enriched = parse_county(county_url, county_html=county_html, delay=0.8)\n",
    "        # stamp county/state context onto each row for downstream grouping/filters\n",
    "        if not df_enriched.empty:\n",
    "            df_enriched = df_enriched.copy()\n",
    "            if \"State\" not in df_enriched.columns:\n",
    "                df_enriched[\"State\"] = state_abbr  # harmonize with existing schema\n",
    "            if \"County\" not in df_enriched.columns:\n",
    "                df_enriched[\"County\"] = county_name\n",
    "        county_frames[key] = df_enriched\n",
    "\n",
    "        print(f\"[OK] {state_abbr} – {county_name}: {len(df_enriched)} rows\")\n",
    "\n",
    "    except Exception as e:\n",
    "        errors.append({\"state_abbr\": state_abbr, \"county_name\": county_name,\n",
    "                       \"county_url\": county_url, \"error\": str(e)[:500]})\n",
    "        print(f\"[ERR] {state_abbr} – {county_name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b155c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Midwest hospitals: (1059, 27)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rifa\\AppData\\Local\\Temp\\ipykernel_22792\\142911147.py:4: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_all_midwest_hospitals = pd.concat(\n"
     ]
    }
   ],
   "source": [
    "if county_frames:\n",
    "    df_all_midwest_hospitals = pd.concat(\n",
    "        [v for v in county_frames.values() if v is not None],\n",
    "        ignore_index=True\n",
    "    )\n",
    "else:\n",
    "    df_all_midwest_hospitals = pd.DataFrame()\n",
    "\n",
    "print(\"All Midwest hospitals:\", df_all_midwest_hospitals.shape)\n",
    "if not df_all_midwest_hospitals.empty:\n",
    "    if \"detail_error\" in df_all_midwest_hospitals.columns:\n",
    "        df_all_midwest_hospitals = df_all_midwest_hospitals[\n",
    "            df_all_midwest_hospitals[\"detail_error\"].isna() | (df_all_midwest_hospitals[\"detail_error\"] == \"\")\n",
    "        ].drop(columns=[\"detail_error\"])\n",
    "    if \"hospital\" in df_all_midwest_hospitals.columns:\n",
    "        df_all_midwest_hospitals = df_all_midwest_hospitals[\n",
    "            df_all_midwest_hospitals[\"hospital\"].notna() & (df_all_midwest_hospitals[\"hospital\"].astype(str).str.strip() != \"\")\n",
    "        ].reset_index(drop=True)\n",
    "df_all_midwest_hospitals.head(20)\n",
    "#export as a CSV\n",
    "df_all_midwest_hospitals.to_csv(\"midwest_hospitals_enriched.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
