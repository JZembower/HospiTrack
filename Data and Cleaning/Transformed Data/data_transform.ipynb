{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2a2f956",
   "metadata": {},
   "source": [
    "## Data transformation file for chicago and US enriched datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4f4f3abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import math\n",
    "import csv\n",
    "import re\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d1b5a69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique states found in dataset:\n",
      "AK\n",
      "AL\n",
      "AR\n",
      "AZ\n",
      "CA\n",
      "CO\n",
      "CT\n",
      "DE\n",
      "FL\n",
      "GA\n",
      "HI\n",
      "IA\n",
      "ID\n",
      "IL\n",
      "IN\n",
      "KS\n",
      "KY\n",
      "LA\n",
      "MA\n",
      "MD\n",
      "ME\n",
      "MI\n",
      "MN\n",
      "MO\n",
      "MS\n",
      "MT\n",
      "NC\n",
      "ND\n",
      "NE\n",
      "NH\n",
      "NJ\n",
      "NM\n",
      "NV\n",
      "NY\n",
      "OH\n",
      "OK\n",
      "OR\n",
      "PA\n",
      "RI\n",
      "SC\n",
      "SD\n",
      "TN\n",
      "TX\n",
      "UT\n",
      "VA\n",
      "VT\n",
      "WA\n",
      "WI\n",
      "WV\n",
      "WY\n"
     ]
    }
   ],
   "source": [
    "#Initial dataset to check to see what states are represented in the US dataset\n",
    "import csv\n",
    "\n",
    "file_path = \"us_hospitals_data_enriched.csv\"\n",
    "states = set()\n",
    "\n",
    "with open(file_path, newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        state = row.get('detail_state', '').strip()\n",
    "        if state:\n",
    "            states.add(state)\n",
    "\n",
    "print(\"Unique states found in dataset:\")\n",
    "for s in sorted(states):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea6432f",
   "metadata": {},
   "source": [
    "Clean CMS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6929b876",
   "metadata": {},
   "outputs": [],
   "source": [
    "#omitting hospitals that are not located in the US and have empty values in the APC_DESC column\n",
    "def extract_clean_rows(input_path: str):\n",
    "    \"\"\"\n",
    "    Keep only rows where Rndrng_Prvdr_State_Abrvtn is a US state.\n",
    "    Writes cleaned_medicare_op.csv in the current directory.\n",
    "    \"\"\"\n",
    "    output_path = \"cleaned_medicare_op.csv\"\n",
    "    with open(input_path, newline=\"\", encoding=\"utf-8\") as fin, \\\n",
    "         open(output_path, \"w\", newline=\"\", encoding=\"utf-8\") as fout:\n",
    "\n",
    "        reader = csv.DictReader(fin)\n",
    "        writer = csv.DictWriter(fout, fieldnames=reader.fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for row in reader:\n",
    "            state = (row.get(\"Rndrng_Prvdr_State_Abrvtn\") or \"\").strip().upper()\n",
    "            if state in states:\n",
    "                writer.writerow(row)\n",
    "\n",
    "    print(f\"Saved -only rows to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993bf55a",
   "metadata": {},
   "source": [
    "Count the number of same procedures a single hospital does and save the highest frequency of the same procedure from each hospital to a new csv with the name of the hospital, state, city and zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "73856109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hospital_top_procedures(\n",
    "    input_path=\"cleaned_medicare_op.csv\",\n",
    "    output_path=\"hospital_top_procedures.csv\"\n",
    "):\n",
    "    \"\"\"\n",
    "    For each hospital, find the most frequent APC_DESC procedure(s).\n",
    "    Hospital identity columns (1-based in the source): 2=name, 3=state, 4=city, 5=address, 7=zip.\n",
    "    Output columns (reordered): name, address, city, state, zip, Top_Procedures.\n",
    "    \"\"\"\n",
    "    # 1-based -> 0-based indexes for hospital identity columns in the source file\n",
    "    NAME_IDX   = 1\n",
    "    STATE_IDX  = 2\n",
    "    CITY_IDX   = 3\n",
    "    ADDR_IDX   = 4\n",
    "    ZIP_IDX    = 6\n",
    "\n",
    "    with open(input_path, newline=\"\", encoding=\"utf-8\") as fin:\n",
    "        reader = csv.reader(fin)\n",
    "        header = next(reader)\n",
    "        try:\n",
    "            apc_idx = next(i for i, h in enumerate(header) if h.strip().lower() == \"apc_desc\")\n",
    "        except StopIteration:\n",
    "            raise RuntimeError(\"Could not find 'APC_DESC' column in the input file.\")\n",
    "\n",
    "    proc_counts = defaultdict(Counter)\n",
    "    first_seen_text = defaultdict(dict)  # hospital_key -> {proc_norm: original_text}\n",
    "\n",
    "    with open(input_path, newline=\"\", encoding=\"utf-8\") as fin:\n",
    "        reader = csv.reader(fin)\n",
    "        _ = next(reader)  # skip header\n",
    "\n",
    "        for row in reader:\n",
    "            if len(row) <= max(apc_idx, ZIP_IDX):\n",
    "                continue\n",
    "\n",
    "            name = (row[NAME_IDX]  or \"\").strip()\n",
    "            state = (row[STATE_IDX] or \"\").strip()\n",
    "            city = (row[CITY_IDX]  or \"\").strip()\n",
    "            addr = (row[ADDR_IDX]  or \"\").strip()\n",
    "            zip_ = (row[ZIP_IDX]   or \"\").strip()\n",
    "\n",
    "            hospital_key = (name, state, city, addr, zip_)\n",
    "\n",
    "            apc_raw = (row[apc_idx] or \"\").strip()\n",
    "            if not apc_raw:\n",
    "                continue\n",
    "\n",
    "            apc_norm = apc_raw.lower()\n",
    "            proc_counts[hospital_key][apc_norm] += 1\n",
    "            first_seen_text[hospital_key].setdefault(apc_norm, apc_raw)\n",
    "\n",
    "    with open(output_path, \"w\", newline=\"\", encoding=\"utf-8\") as fout:\n",
    "        writer = csv.writer(fout)\n",
    "        writer.writerow([\"Hospital_Name\", \"Address\", \"City\", \"State\", \"ZIP\", \"Top_Procedures\"])\n",
    "\n",
    "        for hospital_key, counter in proc_counts.items():\n",
    "            name, state, city, addr, zip_ = hospital_key\n",
    "\n",
    "            if not counter:\n",
    "                top_str = \"data not available\"\n",
    "            else:\n",
    "                max_freq = max(counter.values())\n",
    "                tied = [p for p, c in counter.items() if c == max_freq]\n",
    "                tied_sorted = sorted(\n",
    "                    (first_seen_text[hospital_key][p] for p in tied),\n",
    "                    key=lambda s: s.lower()\n",
    "                )[:3]\n",
    "                top_str = \" | \".join(tied_sorted)\n",
    "\n",
    "            # Reordered output: name, address, city, state, zip\n",
    "            writer.writerow([name, addr, city, state, zip_, top_str])\n",
    "\n",
    "    print(f\" Wrote per-hospital top procedures to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b1bbd7",
   "metadata": {},
   "source": [
    "Append top procedures to a final working csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "125b0464",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#  Normalizers\n",
    "SUFFIX_MAP = {\n",
    "    \"STREET\":\"ST\", \"ST\":\"ST\",\n",
    "    \"ROAD\":\"RD\", \"RD\":\"RD\",\n",
    "    \"AVENUE\":\"AVE\", \"AVE\":\"AVE\",\n",
    "    \"BOULEVARD\":\"BLVD\", \"BLVD\":\"BLVD\",\n",
    "    \"DRIVE\":\"DR\", \"DR\":\"DR\",\n",
    "    \"COURT\":\"CT\", \"CT\":\"CT\",\n",
    "    \"LANE\":\"LN\", \"LN\":\"LN\",\n",
    "    \"HIGHWAY\":\"HWY\", \"HWY\":\"HWY\",\n",
    "    \"PARKWAY\":\"PKWY\", \"PKWY\":\"PKWY\",\n",
    "}\n",
    "\n",
    "NAME_STRIP_WORDS = {\n",
    "    \"HOSPITAL\",\"HOSP\",\"MEDICAL\",\"MED\",\"CENTER\",\"CTR\",\"HEALTH\",\"HLTH\",\n",
    "    \"SYSTEM\",\"SYS\",\"CLINIC\",\"LLC\",\"INC\",\"LTD\"\n",
    "}\n",
    "\n",
    "def _norm_space_upper(s: str) -> str:\n",
    "    s = (s or \"\").strip().upper()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def _norm_zip5(s: str) -> str:\n",
    "    s = (s or \"\").strip()\n",
    "    digits = re.sub(r\"\\D\", \"\", s)\n",
    "    if len(digits) >= 5:\n",
    "        return digits[:5]\n",
    "    return digits.zfill(5) if digits else \"\"\n",
    "\n",
    "def _norm_state(s: str) -> str:\n",
    "    s = _norm_space_upper(s)\n",
    "    m = re.search(r\"\\b([A-Z]{2})\\b\", s)\n",
    "    return m.group(1) if m else s[:2]\n",
    "\n",
    "def _simplify_name(name: str) -> str:\n",
    "    t = _norm_space_upper(name)\n",
    "    # remove punctuation\n",
    "    t = re.sub(r\"[^\\w\\s]\", \" \", t)\n",
    "    # collapse spaces\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    # remove common institution words\n",
    "    tokens = [tok for tok in t.split() if tok not in NAME_STRIP_WORDS]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def _norm_address(addr: str) -> str:\n",
    "    t = _norm_space_upper(addr)\n",
    "    # Drop suite/unit/ste/apt/# fragments\n",
    "    t = re.sub(r\"\\b(APT|UNIT|STE|SUITE|#)\\s*\\w+\\b\", \"\", t)\n",
    "    # Standardize street suffixes at end\n",
    "    # remove punctuation\n",
    "    t = re.sub(r\"[^\\w\\s]\", \" \", t)\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    parts = t.split()\n",
    "    if parts:\n",
    "        last = parts[-1]\n",
    "        # expand/normalize last token\n",
    "        last_norm = SUFFIX_MAP.get(last, SUFFIX_MAP.get(last.rstrip(\".\"), last))\n",
    "        parts[-1] = last_norm\n",
    "    return \" \".join(parts)\n",
    "\n",
    "def _key_full(name, addr, city, state, zip5):\n",
    "    return (\"F\", _norm_space_upper(name), _norm_address(addr),\n",
    "            _norm_space_upper(city), _norm_state(state), _norm_zip5(zip5))\n",
    "\n",
    "def _key_name_zip(name, zip5):\n",
    "    return (\"NZ\", _norm_space_upper(name), _norm_zip5(zip5))\n",
    "\n",
    "def _key_name_city_state(name, city, state):\n",
    "    return (\"NCS\", _norm_space_upper(name), _norm_space_upper(city), _norm_state(state))\n",
    "\n",
    "def _key_addr_zip(addr, zip5):\n",
    "    return (\"AZ\", _norm_address(addr), _norm_zip5(zip5))\n",
    "\n",
    "def _key_simplename_zip(name, zip5):\n",
    "    return (\"SNZ\", _simplify_name(name), _norm_zip5(zip5))\n",
    "\n",
    "# --- Joiner ---\n",
    "def append_top_procedures(\n",
    "    hosp_proc_path=\"hospital_top_procedures.csv\",\n",
    "    er_path=\"us_hospitals_data_enriched.csv\",\n",
    "    output_path=\"US_er_final.csv\"\n",
    "):\n",
    "    # Build multi-index map from hospital_top_procedures.csv\n",
    "    # Expected columns there: Hospital_Name, Address, City, State, ZIP, Top_Procedures\n",
    "    maps = {}  # dict of key -> procedures\n",
    "    def _put(k, v):\n",
    "        if k and v:\n",
    "            maps[k] = v\n",
    "\n",
    "    with open(hosp_proc_path, newline=\"\", encoding=\"utf-8\") as f:\n",
    "        r = csv.DictReader(f)\n",
    "        fn = r.fieldnames or []\n",
    "        need = lambda cands: next((c for c in cands if c in fn), None)\n",
    "\n",
    "        name_c = need([\"Hospital_Name\",\"Name\",\"Hospital\"])\n",
    "        addr_c = need([\"Address\",\"Street\",\"Addr\"])\n",
    "        city_c = need([\"City\",\"Town\"])\n",
    "        state_c = need([\"State\"])\n",
    "        zip_c = need([\"ZIP\",\"Zip\",\"PostalCode\"])\n",
    "        proc_c = need([\"Top_Procedures\",\"Top Procedures\",\"Procedures\"])\n",
    "\n",
    "        if not all([name_c, addr_c, city_c, state_c, zip_c, proc_c]):\n",
    "            raise RuntimeError(\"hospital_top_procedures.csv missing required columns\")\n",
    "\n",
    "        for row in r:\n",
    "            name = row.get(name_c, \"\")\n",
    "            addr = row.get(addr_c, \"\")\n",
    "            city = row.get(city_c, \"\")\n",
    "            state = row.get(state_c, \"\")\n",
    "            zip5 = row.get(zip_c, \"\")\n",
    "            procs = (row.get(proc_c) or \"\").strip() or \"data not available\"\n",
    "\n",
    "            _put(_key_full(name, addr, city, state, zip5), procs)\n",
    "            _put(_key_name_zip(name, zip5), procs)\n",
    "            _put(_key_name_city_state(name, city, state), procs)\n",
    "            _put(_key_addr_zip(addr, zip5), procs)\n",
    "            _put(_key_simplename_zip(name, zip5), procs)\n",
    "\n",
    "    # Read ER file, append Top_Procedures\n",
    "    with open(er_path, newline=\"\", encoding=\"utf-8\") as fin, \\\n",
    "         open(output_path, \"w\", newline=\"\", encoding=\"utf-8\") as fout:\n",
    "        r = csv.DictReader(fin)\n",
    "        if not r.fieldnames:\n",
    "            raise RuntimeError(\"midwest_er_transformed.csv has no header\")\n",
    "\n",
    "        name_c = \"hospital_name\"\n",
    "        addr_c = \"detail_address\"\n",
    "        city_c = \"detail_city\"\n",
    "        state_c = \"detail_state\"\n",
    "        zip_c = \"detail_zip\"\n",
    "        for col in [name_c, addr_c, city_c, state_c, zip_c]:\n",
    "            if col not in r.fieldnames:\n",
    "                raise RuntimeError(f\"Missing required column '{col}' in {er_path}\")\n",
    "\n",
    "        out_col = \"Top_Procedures\"\n",
    "        fieldnames = list(r.fieldnames)\n",
    "        if out_col not in fieldnames:\n",
    "            fieldnames.append(out_col)\n",
    "\n",
    "        w = csv.DictWriter(fout, fieldnames=fieldnames)\n",
    "        w.writeheader()\n",
    "\n",
    "        for row in r:\n",
    "            name = row.get(name_c, \"\")\n",
    "            addr = row.get(addr_c, \"\")\n",
    "            city = row.get(city_c, \"\")\n",
    "            state = row.get(state_c, \"\")\n",
    "            zip5 = row.get(zip_c, \"\")\n",
    "\n",
    "            # Try in order of strictest to looser\n",
    "            keys = [\n",
    "                _key_full(name, addr, city, state, zip5),\n",
    "                _key_name_zip(name, zip5),\n",
    "                _key_name_city_state(name, city, state),\n",
    "                _key_addr_zip(addr, zip5),\n",
    "                _key_simplename_zip(name, zip5),\n",
    "            ]\n",
    "            procs = \"data not available\"\n",
    "            for k in keys:\n",
    "                if k in maps:\n",
    "                    procs = maps[k]\n",
    "                    break\n",
    "\n",
    "            row[out_col] = procs\n",
    "            w.writerow(row)\n",
    "\n",
    "    print(f\"Appended Top_Procedures to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9455f2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _safe_float(x):\n",
    "    try:\n",
    "        return float(x)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def _round_half_up(x):\n",
    "    return int(math.floor(x + 0.5))\n",
    "\n",
    "def _clamp(v, lo, hi):\n",
    "    return max(lo, min(hi, v))\n",
    "\n",
    "# 5% bands: lower mortality is better\n",
    "def _adj_from_mortality_5pct_bands(pct) -> int:\n",
    "    p = _safe_float(pct)\n",
    "    if p is None:\n",
    "        return 0\n",
    "    p = max(0.0, p)\n",
    "    return 1 if p < 5 else -math.ceil((p - 5.0) / 5.0)\n",
    "\n",
    "# parse overall mortality into signed 20%-blocks (± up to 5)\n",
    "_PCT_RE = re.compile(r\"(\\d+(?:\\.\\d+)?)\\s*%\")\n",
    "def _parse_overall_mortality_blocks(r):\n",
    "    direction = (r.get(\"detail_mortality_overall_direction\") or \"\").strip().lower()\n",
    "    pct = _safe_float(r.get(\"detail_mortality_overall_percent\"))\n",
    "    text = (r.get(\"detail_mortality_overall_text\") or \"\").strip().lower()\n",
    "\n",
    "    if direction not in (\"better\", \"worse\"):\n",
    "        if \"better\" in text: direction = \"better\"\n",
    "        elif \"worse\" in text: direction = \"worse\"\n",
    "\n",
    "    if pct is None:\n",
    "        m = _PCT_RE.search(text)\n",
    "        if m:\n",
    "            pct = _safe_float(m.group(1))\n",
    "\n",
    "    if direction in (\"better\", \"worse\") and pct is not None:\n",
    "        blocks = _clamp(_round_half_up(pct / 20.0), 0, 5)\n",
    "        signed = blocks if direction == \"better\" else -blocks\n",
    "        return f\"{_round_half_up(pct)}% {direction} (±{blocks})\", signed\n",
    "\n",
    "    return \"mortality not used\", 0\n",
    "\n",
    "def _ed_points_label(baseline, ed_minutes, wait_minutes):\n",
    "    ed = _safe_float(ed_minutes)\n",
    "    wt = _safe_float(wait_minutes)\n",
    "    if (ed is None) or (wt is None) or (ed == 0) or (wt == 0):\n",
    "        return \"wait time rating not available\", None\n",
    "    pts = 10 - int(max(0.0, ed - baseline) // 30)\n",
    "    if pts > 0:\n",
    "        return str(pts), pts\n",
    "    return \"points not available\", 0\n",
    "\n",
    "_PR_MAP = {\n",
    "    \"very good\": 10, \"good\": 9, \"above average\": 8,\n",
    "    \"average\": 7, \"below average\": 6, \"poor\": 5, \"very poor\": 4\n",
    "}\n",
    "def _patient_points_label(text):\n",
    "    t = (text or \"\").strip().lower()\n",
    "    if t in _PR_MAP:\n",
    "        pts = _PR_MAP[t]\n",
    "        return str(pts), pts\n",
    "    return \"patient rating not available\", None\n",
    "\n",
    "def _fmt_adj_or_same(base_total, delta):\n",
    "    return (\n",
    "        str(base_total + delta)\n",
    "        if delta != 0\n",
    "        else f\"no data for specific chief complaint, quality point remains at {base_total}\"\n",
    "    )\n",
    "\n",
    "# ---------- main enrichment ----------\n",
    "def add_quality_points(in_path: str, out_path: str) -> None:\n",
    "    with open(in_path, newline=\"\", encoding=\"utf-8\") as f:\n",
    "        rows = list(csv.DictReader(f))\n",
    "\n",
    "    # baseline ED minutes: prefer row 2 if valid; else min valid across file\n",
    "    base2 = _safe_float(rows[1].get(\"detail_avg_time_in_ed_minutes\")) if len(rows) >= 2 else None\n",
    "    if base2 is not None and base2 > 0:\n",
    "        baseline = base2\n",
    "    else:\n",
    "        candidates = [\n",
    "            _safe_float(rr.get(\"detail_avg_time_in_ed_minutes\"))\n",
    "            for rr in rows\n",
    "            if _safe_float(rr.get(\"detail_avg_time_in_ed_minutes\")) not in (None, 0)\n",
    "            and _safe_float(rr.get(\"wait_minutes\")) not in (None, 0)\n",
    "        ]\n",
    "        baseline = min(candidates) if candidates else 0.0\n",
    "\n",
    "    prelim, ed_valid, patient_valid = [], [], []\n",
    "    for r in rows:\n",
    "        ed_label, ed_pts = _ed_points_label(baseline, r.get(\"detail_avg_time_in_ed_minutes\"), r.get(\"wait_minutes\"))\n",
    "        if isinstance(ed_pts, int): ed_valid.append(ed_pts)\n",
    "\n",
    "        pr_label, pr_pts = _patient_points_label(r.get(\"detail_overall_patient_rating\"))\n",
    "        if isinstance(pr_pts, int): patient_valid.append(pr_pts)\n",
    "\n",
    "        mort_label, mort_signed = _parse_overall_mortality_blocks(r)\n",
    "        prelim.append((r, ed_label, ed_pts, pr_label, pr_pts, mort_label, mort_signed))\n",
    "\n",
    "    ed_mean = _round_half_up(sum(ed_valid) / len(ed_valid)) if ed_valid else 0\n",
    "    pr_mean = _round_half_up(sum(patient_valid) / len(patient_valid)) if patient_valid else 0\n",
    "\n",
    "    # headers (drop legacy; add new)\n",
    "    fieldnames = list(rows[0].keys()) if rows else []\n",
    "    for legacy in (\"adjusted_qp_heartattack\",\"adjusted_qp_stroke\",\"adjusted_qp_heartfailure\",\"adjusted_qp_pneu\",\"base_total_quality_points\"):\n",
    "        if legacy in fieldnames: fieldnames.remove(legacy)\n",
    "    for c in (\n",
    "        \"ed_minutes_rating\",\n",
    "        \"detail_overall_patient_rating_points\",\n",
    "        \"mortality_overall_contribution\",\n",
    "        \"total_quality_points\",\n",
    "        \"adj_total_heartattack\",\n",
    "        \"adj_total_stroke\",\n",
    "        \"adj_total_heartfailure\",\n",
    "        \"adj_total_pneu\",\n",
    "    ):\n",
    "        if c not in fieldnames: fieldnames.append(c)\n",
    "\n",
    "    out_rows = []\n",
    "    for r, ed_label, ed_pts, pr_label, pr_pts, mort_label, mort_signed in prelim:\n",
    "        ed_pts = ed_mean if ed_pts is None else ed_pts\n",
    "        pr_pts = pr_mean if pr_pts is None else pr_pts\n",
    "\n",
    "        base_total = int(ed_pts) + int(pr_pts) + int(mort_signed)\n",
    "        r[\"ed_minutes_rating\"] = ed_label\n",
    "        r[\"detail_overall_patient_rating_points\"] = pr_label\n",
    "        r[\"mortality_overall_contribution\"] = mort_label\n",
    "        r[\"total_quality_points\"] = str(base_total)\n",
    "\n",
    "        # per-condition deltas (non-cumulative)\n",
    "        deltas = {\n",
    "            \"heartattack\": _adj_from_mortality_5pct_bands(r.get(\"detail_mortality_heart_attack_percent\")),\n",
    "            \"stroke\":      _adj_from_mortality_5pct_bands(r.get(\"detail_mortality_stroke_percent\")),\n",
    "            \"heartfailure\":_adj_from_mortality_5pct_bands(r.get(\"detail_mortality_heart_failure_percent\")),\n",
    "            \"pneu\":        _adj_from_mortality_5pct_bands(r.get(\"detail_mortality_pneumonia_percent\")),\n",
    "        }\n",
    "        r[\"adj_total_heartattack\"] = _fmt_adj_or_same(base_total, deltas[\"heartattack\"])\n",
    "        r[\"adj_total_stroke\"] = _fmt_adj_or_same(base_total, deltas[\"stroke\"])\n",
    "        r[\"adj_total_heartfailure\"] = _fmt_adj_or_same(base_total, deltas[\"heartfailure\"])\n",
    "        r[\"adj_total_pneu\"] = _fmt_adj_or_same(base_total, deltas[\"pneu\"])\n",
    "\n",
    "        out_rows.append(r)\n",
    "\n",
    "    with open(out_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        csv.DictWriter(f, fieldnames=fieldnames).writeheader()\n",
    "        csv.DictWriter(f, fieldnames=fieldnames).writerows(out_rows)\n",
    "\n",
    "# ---------- complaint-aware ranking (read-only) ----------\n",
    "_COMPLAINT_TO_COL = {\n",
    "    \"chest pain\": [\"detail_mortality_heart_attack_percent\"],\n",
    "    \"heart attack\": [\"detail_mortality_heart_attack_percent\", \"detail_mortality_overall_percent\"],\n",
    "    \"slurred speech\": [\"detail_mortality_stroke_percent\"],\n",
    "    \"facial droop\": [\"detail_mortality_stroke_percent\"],\n",
    "    \"stroke\": [\"detail_mortality_stroke_percent\"],\n",
    "    \"shortness of breath\": [\"detail_mortality_heart_failure_percent\", \"detail_mortality_pneumonia_percent\"],\n",
    "    \"trouble breathing\": [\"detail_mortality_heart_failure_percent\", \"detail_mortality_pneumonia_percent\"],\n",
    "    \"cough\": [\"detail_mortality_pneumonia_percent\"],\n",
    "    \"fever\": [\"detail_mortality_pneumonia_percent\"],\n",
    "    \"default\": [\"detail_mortality_overall_percent\"],\n",
    "}\n",
    "\n",
    "def _pick_mort_col(complaint: str):\n",
    "    c = (complaint or \"\").strip().lower()\n",
    "    for k, cols in _COMPLAINT_TO_COL.items():\n",
    "        if k != \"default\" and k in c:\n",
    "            return cols\n",
    "    return _COMPLAINT_TO_COL[\"default\"]\n",
    "\n",
    "def _mortality_points_0to5(pct) -> int | None:\n",
    "    p = _safe_float(pct)\n",
    "    if p is None:\n",
    "        return None\n",
    "    p = _clamp(p, 0.0, 100.0)\n",
    "    return max(0, 5 - int(p // 20))\n",
    "\n",
    "def complaint_points_for_row(row: dict, complaint: str) -> tuple[int, str]:\n",
    "    for col in _pick_mort_col(complaint):\n",
    "        pts = _mortality_points_0to5(row.get(col))\n",
    "        if pts is None:\n",
    "            continue\n",
    "        p = _safe_float(row.get(col))\n",
    "        return pts, f\"{col}→{(f'{p:.1f}%' if p is not None else 'NA')} ⇒ +{pts}\"\n",
    "    return 0, \"complaint mortality not used\"\n",
    "\n",
    "def rank_with_complaint(enriched_path: str, complaint: str, top_k: int = 10) -> list[dict]:\n",
    "    with open(enriched_path, newline=\"\", encoding=\"utf-8\") as f:\n",
    "        rows = list(csv.DictReader(f))\n",
    "\n",
    "    results = []\n",
    "    for r in rows:\n",
    "        base = int(_safe_float(r.get(\"total_quality_points\")) or 0)\n",
    "        cmp_pts, explain = complaint_points_for_row(r, complaint)\n",
    "        results.append({\n",
    "            \"name\": r.get(\"name\",\"\"),\n",
    "            \"city\": r.get(\"city\",\"\"),\n",
    "            \"complaint\": complaint,\n",
    "            \"complaint_mortality_points\": cmp_pts,\n",
    "            \"complaint_mortality_explain\": explain,\n",
    "            \"complaint_total_quality_points\": base + int(cmp_pts),\n",
    "        })\n",
    "\n",
    "    results.sort(key=lambda x: x[\"complaint_total_quality_points\"], reverse=True)\n",
    "    return results[:max(1, top_k)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fe03a72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved -only rows to cleaned_medicare_op.csv\n"
     ]
    }
   ],
   "source": [
    "#Call and create new cleaned csv for US for medicare outpatient data\n",
    "extract_clean_rows(\"Medicare_OP_Hospitals_by_Provider_and_Service_2023.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dc9966f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Wrote per-hospital top procedures to hospital_top_procedures.csv\n"
     ]
    }
   ],
   "source": [
    "#Call hospital top procedures\n",
    "hospital_top_procedures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c5474e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended Top_Procedures to US_er_final.csv\n"
     ]
    }
   ],
   "source": [
    "#Call to append top procedures to original US csv. Create a final master csv for front end usage. \n",
    "append_top_procedures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "835f2248",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call and create a new tranformed us_hospitals_enriched.csv called us_er_transformed.csv\n",
    "add_quality_points(\n",
    "    in_path=\"us_hospitals_data_enriched.csv\",\n",
    "    out_path=\"us_er_transformed.csv\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
